\documentclass{article}

\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\title{Deep Learning --- Assignment 3}

\author{Anirudhan J.~Rajagopalan \\
  Department of Computer Science\\
  New York University\\
  New York, NY.\\
  \texttt{ajr619@nyu.edu} \\
}

\begin{document}

\maketitle

\section{General Questions}
\subsection{Distilling a network into one module}
A linear activation function can be defined as 
\begin{align*}
  a^{{i}} = w^{(i)}x + b^{i}
\end{align*}

A linear combination of such layers can be expressed as a sum of all the individual activations.  For example, given two activation layers
\begin{align*}
  y =& w^{(2)}x_{out} + b^{(2)}\\
  =& w^{(2)}w^{(1)}x_{in} + w^{(2)}b^{(1)} + b^{(2)}
\end{align*}
Generalizing this, we can write
\begin{align*}
  y =& Wx_{in} + b \\
  W =& \prod_{i = 1}^{l} w_{i} \\
  b =& \sum_{i = 1}^{l} (\prod_{l}^{i+1} w_{i})b_{i} + b_{n}
\end{align*}

\subsection{Dictionary used in Sparse coding vs Autoencoders}
An autoencoder is a model which tries to reconstruct the input using some sort of constraint.  Without any constraints an autoencoder can learn just the identity function which will will always provide the least reconstruction loss.  Thus a dictionary learnt by an autoencoder need not be sparse.  Additionally there are different type of encoding techniques, like denoising autoencoders which can be used to remove noise from an input.
Sparse coding is a encoding method which can forces the model to find a sparse and overcomplete representation of the input.  Thus a dictionary learnt with denoisign autoencoder will always be sparse (there will be lot of features with zeros).


\section{Softmax regression gradient calculation}
\subsection{Derive $\frac{\partial l}{\partial W_{i,j}}$}
\subsection{Loss functions and gradients}
\subsubsection{When $y_{c1} = 1, \hat{y_{c2}} = 1, c_{1} \ne c_{2}$}
\subsubsection{Why there is no need to worry about this situation}

\section{Chain rule}
\subsection{Calculating gradients using chain rule}
\subsection{Gradients at $x = 1, y = 0$}

\section{Variants of Pooling}
Pooling is an operation in Convolutional Neural Networks which is performed after a convolutional layer.  
We decide on a size of the region to do pool our convolutional filters over.
Then we divide the convolved features into disjoint regions and take the max, mean or norm of the feature activations depending on the type of pooling operation.
Generally the stride length used in pooling is equal to the pooling window size.
\subsection{Three types of pooling}
There are three types of pooling operations that are used
\begin{description}
  \item[Max pooling] Max pooling takes the maximum of the feature activations.  Implemented by \mbox{SpatialMaxPooling}.
  \item[Average pooling] Average pooling takes the average of the feature activations. Implemented by \mbox{SpatialAveragePooling}.
  \item[Lp pooling] LpPooling takes the lp norm of the feature activations. Implemented by \mbox{SpatialLPPooling}.
\end{description}

\subsection{Mathematical formulas for pooling}
Suppose we have a $N x N$ convolutional layer (filters) and a pooling layer of size $m x m$ where $ m < N $
The pooling operation is generally implemented using a stride of m.
Therefore the size of filters after pooling will be $\frac{N}{m} * \frac{N}{m} $ and each of the $m x m$ pooling operations gives a single value.
The single value for a pooling operation can be represented by
\begin{align*}
  y = \sigma(x_{ij})
\end{align*}
where $1 \le i \le m $ and $1 \le j \le m $.
\subsubsection{MaxPooling}
Incase of max pooling
\begin{align*}
  \sigma = \max(x_{ij}) \qquad 1 \le i \le m and 1 \le j \le m
\end{align*}

\subsubsection{Average Pooling}
Incase of average pooling
\begin{align*}
  \sigma = \frac{\sum_{i=1}^{m} \sum_{j=1}^{m} x_{ij}}{m*m}
\end{align*}

\subsubsection{LPPooling}
Incase of LP pooling where $p \ge 2$.
\begin{align*}
  \sigma = {(\sum_{i=1}^{m}\sum_{j=1}^{m} x_{ij}^{p} )}^{\frac{1}{p}}
\end{align*}

\subsection{Reason for using in deep learning}
Pooling is used for a number of reasons.
\begin{enumerate}
  \item Computational tractability.  The number of features after convolution increases by $numFIlters x (N-m+1) x (N-m+1)$.  Generally it will be computationally intractable to perform learning on this huge data.
  \item Overfitting.  Since the number of features are high, this will lead to overfitting.  Pooling helps in reducing the number of features.
  \item Stationary property.  Generally images have stationary property.  This means that features that are useful in one region will be useful in other regions also.  Pooling operation helps in aggregating the features at various locations.
\end{enumerate}

We generally use Max-pooling for images as the performance of Max pooling has been emperically shown to be better than Average or Lp pooling for image workflows.  This can be explained by the stationary property of the image as the most important activation in a small patch can be found by using max pooling.

\section{Convolution}
\subsection{Number of values after forward propogation}
When we apply a convolution on a $N \times N$ image with a kerenel of size $ m \times m $ we get $(N-m+1) \times (N-m+1)$ output values after the convolution.

Here, N = 5, m = 3.
Therefore we will get $(5-3+1) \times (5-3+1) = 9$ values.
\subsection{Values after forward propogation}
The resulting filter obtained by applying the convolutional kernel is:
\begin{centering}
  \begin{align*}
  109 &\qquad 92 \qquad 72 \\
  108 &\qquad 85 \qquad 74 \\
  110 &\qquad 74 \qquad 79
  \end{align*}
\end{centering}

\subsection{Value of gradient}
\begin{center}
\begin{tabular}{c c c c c }
   4 &   7  &  10  &   6  &   3 \\
   9 &  17  &  25 &   16 &    8 \\
  11 & 23  &  34 &   23 &   11 \\
   7 & 16 &   24 &   17 &    8 \\
   2 &  6 &    9 &    7 &    3
\end{tabular}
\end{center}

\section{Optimization}
\subsection{Mathematical formula for reconstruction loss}
\subsection{Gradient of the loss with respect to the loss}
\subsection{Gradient descent step}
\subsection{Gradient step with momentum}

\section{Top-k error}
\subsection{Definition}
Top-k error is an error metric used in ranking problems.  
This is the number of samples, $x_{i}$ with correct labels $ y_{i} $ that doesn't appear in the top-k predicted results of the model in the order of the probability measure or score used for ranking.  
According to LVSRC 2014 by Imagenet~\cite{ILSVRC15}, the error is defined as
\begin{align*}
  e =& \frac{1}{n}. \sum_{k} \min_{i} d(c_{i}, C_{k})
\end{align*}
where $C_{k}$ is the ground truth of the image with k = 1, \ldots, n labels.\ 
and $c_{i}$ is the labels generated by the classification algorithm with i $\in$ 1, \ldots 5.

Given this formulation, using the top-5 error helps us not to penalize a classification algorithm if it finds an object in an image which is not present in ground truth.

Using top-1 error ensures how close the object identified in the image by an algorithm is with respect to the original image in imagenet database.

\section{t-SNE}
\subsection{Crowding problem}
\subsection{Derive $\frac{\partial C}{\partial y_{i}}$}

\section{Proximal gradient descent}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
