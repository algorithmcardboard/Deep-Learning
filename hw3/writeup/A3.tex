\documentclass{article}

\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}

\title{Deep Learning --- Assignment 3}

\author{Anirudhan J.~Rajagopalan \\
  Department of Computer Science\\
  New York University\\
  New York, NY.\\
  \texttt{ajr619@nyu.edu} \\
}

\begin{document}

\maketitle

\section{General Questions}
\subsection{Distilling a network into one module}
A linear activation function can be defined as 
\begin{align*}
  a^{{i}} = w^{(i)}x + b^{i}
\end{align*}

A linear combination of such layers can be expressed as a sum of all the individual activations.  For example, given two activation layers
\begin{align*}
  y =& w^{(2)}x_{out} + b^{(2)}\\
  =& w^{(2)}w^{(1)}x_{in} + w^{(2)}b^{(1)} + b^{(2)}
\end{align*}
Generalizing this, we can write
\begin{align*}
  y =& Wx_{in} + b \\
  W =& \prod_{i = 1}^{l} w_{i} \\
  b =& \sum_{i = 1}^{l} (\prod_{l}^{i} w_{i})b_{i-1} + b_{n}
\end{align*}

\subsection{Dictionary used in Sparse coding vs Autoencoders}
An autoencoder is a model which tries to reconstruct the input using some sort of constraint.  Without any constraints an autoencoder can learn just the identity function which will will always provide the least reconstruction loss.  Thus a dictionary learnt by an autoencoder need not be sparse.  Additionally there are different type of encoding techniques, like denoising autoencoders which can be used to remove noise from an input.
Sparse coding is a encoding method which can forces the model to find a sparse and overcomplete representation of the input.  
Thus a dictionary learnt with denoising autoencoder will always be sparse (there will be lot of features with zeros).


\section{Softmax regression gradient calculation}
\subsection{Derive $\frac{\partial l}{\partial W_{i,j}}$}
Given:
\begin{align*}
  \hat{y} =& \sigma{(Wx+b)} \\
  \sigma{(a)}_{i} =& \frac{\exp(a_{i})}{\sum_{j} \exp(a_{i})} \\
  l(y, \hat{y}) =& -\sum_{i} y_{i} \log \hat{y}_{i}
\end{align*}

By using chain rule,
\begin{align*}
  \frac{\partial l}{\partial W_{ij}} = \frac{\partial l}{\partial \sigma{(a)}{k}}\frac{\partial\sigma{(a)}_{k}}{\partial W_{ij}}
\end{align*}
also,
\begin{align*}
  \frac{\partial l}{\partial \sigma{(a)}_{k}} =&  -\frac{yk}{\sigma{(a)}_{k}} \\
  \frac{\partial a_{l}}{\partial W_{ij}} =&  \delta_{il}x_{j} \\
  \implies \frac{\partial \sigma{(a)}_{k}}{\partial W_{ij}} =& \frac{\partial \sigma{(a)}_{k}}{\partial {(a)}_{l}} \frac{\partial {(a)}_{l}}{\partial W_{ij}} \\
  =& \frac{\partial \sigma{(a)}_{k}}{\partial {(a)}_{l}} \delta_{il}x_{j} \\
  =& \frac{\partial \sigma{(a)}_{k}}{\partial {(a)}_{i}} x_{j}
\end{align*}
Using chain rule:
\begin{align*}
  \frac{\partial l}{\partial W_{ij}} =& -\frac{yk}{\sigma{(a)}_{k}} \frac{\partial \sigma{(a)}_{k}}{\partial {(a)}_{i}} x_{j} 
\end{align*}
By using the equation obtained in assignment (2).
\begin{align*}
  \frac{\partial \sigma{(a)}_{k}}{\partial {(a)}_{i}} =& \sigma{(a)}_{k} (\delta_{ik} - \sigma{(a)}_{i})
\end{align*}
By substituting $y_{i} = \sigma{(a)}_{i}$
\begin{align*}
  \frac{\partial l}{\partial W_{ij}} =& -\frac{yk}{\sigma{(a)}_{k}} \sigma{(a)}_{k} (\delta_{ik} - \sigma{(a)}_{i}) \\
  =& -yk (\delta_{ik} - \sigma{(a)}_{i})x_{j} \\
  =& (\hat{y}_{i} - y_{i})x_{j}
\end{align*}

\subsection{Loss functions and gradients}
\subsubsection{When $y_{c1} = 1, \hat{y_{c2}} = 1, c_{1} \ne c_{2}$}
When $y_{c1} = 1$, 
\begin{align*}
  l =& -\log(\hat{y}_{c1}) \\
  \frac{\partial l}{\partial W_{ij}} =& -(\delta_{ic1} - \hat{y}_{i})x_{j}
\end{align*}
When $\hat{y_{c2}} = 1, c_{1} \ne c_{2}$, 
\begin{align*}
  \frac{\partial l}{\partial W_{c1j}} =& -x_{j} \\
  \frac{\partial l}{\partial W_{c2j}} =& x_{j} \\
\end{align*}

\subsubsection{Why there is no need to worry about this situation}
Since the weights of the hidden layers are initialized randomly, the possibility of $\hat{y}_{c2} = 1 $ is negligible.  When the weights changes during learning, it moves away from this point.  Hencec we need not worry about this situation.

\section{Chain rule}
\subsection{Calculating gradients using chain rule}
Given
\begin{align*}
  f(x,y) = \frac{x^{2} + \sigma{(y)}}{3x + y - \sigma{(x)}}
\end{align*}
This can be considered as a equation in $x, y, \sigma{(x)}, \sigma{(x)}$
\begin{align*}
  f(x,y)=& E(x,y,\sigma(x),\sigma(y)) \\
\end{align*}
The gradient calculated using the chain rule is:
\begin{align*}
  \frac{\partial f}{\partial x}=& \frac{\partial E}{\partial x}+  \frac{\partial E}{\partial \sigma (x)}  \frac{\partial  \sigma (x)}{\partial x}\\
  \frac{\partial f}{\partial y}=& \frac{\partial E}{\partial y}+  \frac{\partial E}{\partial \sigma (y)}  \frac{\partial  \sigma (y)}{\partial y}
\end{align*}
Where:
\begin{align*}
  \frac{\partial E}{\partial x}=& \frac{2x(3x+y-\sigma{(x)})-(x^2+\sigma{(y)})3}{{(3x+y-\sigma{(x)})}^2}\\
  \frac{\partial E}{\partial y}=& -\frac{x^2+\sigma(y)}{{(3x+y-\sigma(x))}^2}\\
  \frac{\partial E}{\partial \sigma (x)}=& \frac{x^2+\sigma (y)}{{(3x+y-\sigma(x))}^2}\\
  \frac{\partial E}{\partial \sigma (y)} =& \frac{1}{(3x+y-\sigma(x))}\\
  \frac{\partial  \sigma(x)}{\partial x}=&\frac{e^{-x}}{{(1+e^{-x})}^2}\\
  \frac{\partial  \sigma(y)}{\partial y}=&\frac{e^{-y}}{{(1+e^{-y})}^2}
\end{align*}

\subsection{Gradients at $x = 1, y = 0$}
\begin{align*}
  \frac{\partial E}{\partial x}(x=1,y=0)=& 0.007\\
  \frac{\partial E}{\partial y}(x=1,y=0)=&-0.29\\
  \frac{\partial E}{\partial \sigma (x)}=& 0.29\\
  \frac{\partial E}{\partial \sigma (y)} =& 0.44\\
  \frac{\partial  \sigma (x)}{\partial x}=&0.19\\
  \frac{\partial  \sigma (y)}{\partial y}=&0.25
\end{align*}
Substituting the gradients:
\begin{align*}
  \frac{\partial f}{\partial x}=& 0.064\\
  \frac{\partial f}{\partial y}=& -0.18 
\end{align*}

\section{Variants of Pooling}
Pooling is an operation in Convolutional Neural Networks which is performed after a convolutional layer.  
We decide on a size of the region to do pool our convolutional filters over.
Then we divide the convolved features into disjoint regions and take the max, mean or norm of the feature activations depending on the type of pooling operation.
Generally the stride length used in pooling is equal to the pooling window size.
\subsection{Three types of pooling}
There are three types of pooling operations that are used
\begin{description}
  \item[Max pooling] Max pooling takes the maximum of the feature activations.  Implemented by \mbox{SpatialMaxPooling}.
  \item[Average pooling] Average pooling takes the average of the feature activations. Implemented by \mbox{SpatialAveragePooling}.
  \item[Lp pooling] LpPooling takes the lp norm of the feature activations. Implemented by \mbox{SpatialLPPooling}.
\end{description}

\subsection{Mathematical formulas for pooling}
Suppose we have a $N x N$ convolutional layer (filters) and a pooling layer of size $m x m$ where $ m < N $
The pooling operation is generally implemented using a stride of m.
Therefore the size of filters after pooling will be $\frac{N}{m} * \frac{N}{m} $ and each of the $m x m$ pooling operations gives a single value.
The single value for a pooling operation can be represented by
\begin{align*}
  y = \sigma(x_{ij})
\end{align*}
where $1 \le i \le m $ and $1 \le j \le m $.
\subsubsection{MaxPooling}
Incase of max pooling
\begin{align*}
  \sigma = \max(x_{ij}) \qquad 1 \le i \le m and 1 \le j \le m
\end{align*}

\subsubsection{Average Pooling}
Incase of average pooling
\begin{align*}
  \sigma = \frac{\sum_{i=1}^{m} \sum_{j=1}^{m} x_{ij}}{m*m}
\end{align*}

\subsubsection{LPPooling}
Incase of LP pooling where $p \ge 2$.
\begin{align*}
  \sigma = {(\sum_{i=1}^{m}\sum_{j=1}^{m} x_{ij}^{p} )}^{\frac{1}{p}}
\end{align*}

\subsection{Reason for using in deep learning}
Pooling is used for a number of reasons.
\begin{enumerate}
  \item Computational tractability.  The number of features after convolution increases by $numFIlters x (N-m+1) x (N-m+1)$.  Generally it will be computationally intractable to perform learning on this huge data.
  \item Overfitting.  Since the number of features are high, this will lead to overfitting.  Pooling helps in reducing the number of features.
  \item Stationary property.  Generally images have stationary property.  This means that features that are useful in one region will be useful in other regions also.  Pooling operation helps in aggregating the features at various locations.
\end{enumerate}

We generally use Max-pooling for images as the performance of Max pooling has been emperically shown to be better than Average or Lp pooling for image workflows.  This can be explained by the stationary property of the image as the most important activation in a small patch can be found by using max pooling.

\section{Convolution}
\subsection{Number of values after forward propogation}
When we apply a convolution on a $N \times N$ image with a kerenel of size $ m \times m $ we get $(N-m+1) \times (N-m+1)$ output values after the convolution.

Here, N = 5, m = 3.
Therefore we will get $(5-3+1) \times (5-3+1) = 9$ values.
\subsection{Values after forward propogation}
The values of filters were hand calculated and verified by the code in~\cite{website:convolution_code}.
The resulting filter obtained by applying the convolutional kernel is:

\begin{centering}
  \begin{align*}
  109 &\qquad 92 \qquad 72 \\
  108 &\qquad 85 \qquad 74 \\
  110 &\qquad 74 \qquad 79
  \end{align*}
\end{centering}

\subsection{Value of gradient}
The values of gradient were hand calculated and verified by the code in~\cite{website:convolution_code}
\begin{center}
\begin{tabular}{c c c c c }
   4 &   7  &  10  &   6  &   3 \\
   9 &  17  &  25 &   16 &    8 \\
  11 & 23  &  34 &   23 &   11 \\
   7 & 16 &   24 &   17 &    8 \\
   2 &  6 &    9 &    7 &    3
\end{tabular}
\end{center}

\section{Optimization}
\subsection{Mathematical formula for reconstruction loss}
An autoencoder is a model which tries to reconstruct the input.  This reconstruction loss for real valued input is generally given by 
\begin{align*}
  L =& \frac{1}{2} \sum_{k}{\lvert\lvert \hat{x}_{k} - x_{k} \rvert\rvert}^{2} \\
  \hat{x}_{k} =& \sigma_{2}(W'\sigma_{1}(WX + b) + b')
\end{align*}

Here, $\sigma_{2} $ denotes the decoding function and $ \sigma $ denotes the encoding function.

When the input are real valued, we can use cross entropy loss.
\begin{align*}
  L(X, Z) = -\sum_{k = 1}^{d} [X_{k}\log{Z_{k}} + (1-X_{k}) \log{(1 - Z_{k})} ]
\end{align*}

\subsection{Gradient of the loss with respect to the loss}
\begin{align*}
 A_1 = &Wx\\
 A_2 = &W^*(\sigma_1{A_1}) \\
\frac{\partial l}{\partial W} = & \frac{\partial l}{\partial \sigma_2}
\frac{\partial \sigma_2}{\partial A_2}
\frac{\partial A_2}{\partial \sigma_1}
\frac{\partial \sigma_1}{\partial A_1}\frac{\partial A_1}{\partial W}
\\
\frac{\partial l}{\partial W^*} = & \frac{\partial l}{\partial \sigma_2}
\frac{\partial \sigma_2}{\partial A_2}
\frac{\partial A_2}{\partial W^*} \\
\frac{\partial l}{\partial W} = & -2(x-\sigma_2(A_2))\frac{\partial \sigma_2}{\partial A_2}W^*\frac{\partial \sigma_1}{\partial A_1}x
\\
\frac{\partial l}{\partial W^*} = &-2(x-\sigma_2(A_2))\frac{\partial \sigma_2}{\partial A_2}(\sigma{A_1})
\end{align*}
\subsection{Gradient descent step}
The gradient descent is eta $\eta $ multiplied by the gradient of loss with respect to the parameters
\begin{align*}
 w*^{t} =& w*^{t-1} - \eta \frac{\partial l}{\partial W*^{t-1}}  \\
 w^{t} =& w^{t-1} - \eta \frac{\partial l}{\partial W^{t-1}}
\end{align*}
\subsection{Gradient step with momentum}
Momentum is used to help the network out of local minimas. By adding the momentum term, the udpate step is now
\begin{align*}
 w*^{t} =& w*^{t-1} - \eta \frac{\partial l}{\partial W*^{t-1}} - m \times \eta \frac{\partial l^{t-1}}{\partial W*^{t-2}} \\
 w^{t} =& w^{t-1} - \eta \frac{\partial l}{\partial W^{t-1}} - m \times \eta \frac{\partial l^{t-1}}{\partial W^{t-2}} \\
\end{align*}
for a fixed eta.

\section{Top-k error}
\subsection{Definition}
Top-k error is an error metric used in ranking problems.  
This is the number of samples, $x_{i}$ with correct labels $ y_{i} $ that doesn't appear in the top-k predicted results of the model in the order of the probability measure or score used for ranking.  
According to LVSRC 2014 by Imagenet~\cite{ILSVRC15}, the error is defined as
\begin{align*}
  e =& \frac{1}{n}. \sum_{k} \min_{i} d(c_{i}, C_{k})
\end{align*}
where $C_{k}$ is the ground truth of the image with k = 1, \ldots, n labels.\ 
and $c_{i}$ is the labels generated by the classification algorithm with i $\in$ 1, \ldots 5.

Given this formulation, using the top-5 error helps us not to penalize a classification algorithm if it finds an object in an image which is not present in ground truth.

Using top-1 error ensures how close the object identified in the image by an algorithm is with respect to the original image in imagenet database.

\section{t-SNE}
\subsection{Crowding problem}
The volume of a sphere centered on datapoint i scales as r and m, where r is the radius and m the dimensionality of the sphere.  If the data is uniformly distributed around i in the high dimensional manifold, and when we try to project them into two dimensional manifold, the area in the lower dimensional manifold available to map distant data points will not be large enough compared to the area available to map nearby datapoints.  This leads to a large number of data points collecting to the center of the map.  This is called as crowding problem.

In t-SNE we solve this problem by using gaussian distribution for high dimensional data and student-t distribution for the two dimensional data.~\cite{van2008visualizing}

\subsection{Derive $\frac{\partial C}{\partial y_{i}}$}
\begin{align}
\label{C}
C=\sum_i\sum_j p_{ij}\log \frac{p_{ij}}{q_{ij}}= \sum_i\sum_j (p_{ij}\log p_{ij}-p_{ij}\log q_{ij})
\end{align}

Lets define $d_{ij}=|| y_i -y_j ||$, then the expression of $q_{ij}$ is:
\begin{equation}\label{q}
q_{ij} =\frac{e^{-d_{ij}^2}}{\sum_{k\neq l} e^{-d_{kl}^2}}
\end{equation}

the dependency of $C$ from $y_i$ is going to be only through $d_{ij}^2$ and $d_{ji}^2$:
 $$
 \frac{\partial C}{\partial y_i}= \sum_j( \frac{\partial C}{\partial d_{ij}^2}\frac{\partial{d_{ij}^2}}{\partial y_i}+ \frac{\partial C}{\partial d_{ji}^2}\frac{\partial{d_{ji}^2}}{\partial y_i})
 $$
Because of the symmetry for exchange of $y_i$ and $y_j$ of $d_{ij}^2$:
 \begin{equation}
 \frac{\partial C}{\partial y_i}= 4\sum_j \frac{\partial C}{\partial d_{ij}^2}(y_i-y_j)
 \end{equation}\label{Cgrad}
 Let:
 \begin{equation}\label{Z}
 Z=(\sum_{k\neq l} e^{-d_{kl}^2})
 \end{equation}
 And than from (\ref{q}):
 \begin{equation}\label{Zq}
 q_{ij} =\frac{e^{-d_{ij}^2}}{Z}\,\Rightarrow \,  q_{ij}Z=e^{-d_{ij}^2}
 \end{equation}
 The first part of (\ref{C}) is a constant with respect to $d_{ij}$ so:
 $$
 \frac{\partial C}{\partial d_{ij}^2}=-\sum_{k\neq l} p_{kl}\frac{\partial(\log q_{kl})}{\partial d_{ij}^2}=
 -\sum_{k\neq l} p_{kl}\frac{\partial(\log q_{kl}Z - \log Z)}{\partial d_{ij}^2}
 $$
 where we have multiply and divided $p_{kl}$  for $Z$ and split the logarithm. Using the (\ref{Zq}):
 \begin{equation}\label{partialC2}
  \frac{\partial C}{\partial d_{ij}^2}=-\sum_{k\neq l} p_{kl}[(\frac{1}{q_{kl}Z})\frac{\partial e^{-d_{kl}^2}}{\partial d_{ij}^2} -\frac{1}{Z}\frac{\partial Z}{\partial d_{ij}^2}]
 \end{equation}
 Calculating the two derivative in this equation:
  \begin{eqnarray*}
 \frac{\partial e^{-d_{kl}^2}}{\partial d_{ij}^2} & = & -e^{-d_{ij}^2} \delta_{ik}\delta_{jl}\\
 \frac{\partial Z}{\partial d_{ij}^2}& = & -e^{-d_{ij}^2} 
  \end{eqnarray*}
where we used the Kronecker delta   $\delta_{ik}$ that is $1$ for $i=k$ and $0$ otherwise. Than substituting in (\ref{partialC2}) the only terms of the sum that remains is the one with $i=k$ and $j=l$:
$$
  \frac{\partial C}{\partial d_{ij}^2} = p_{ij}(\frac{1}{q_{ij}Z}) e^{-d_{ij}^2} -
  \sum_{k \neq l} p_{kl}\frac{1}{Z}e^{-d_{ij}^2}=(p_{ij}- q_{ij})$$
where we used the (\ref{Zq}) and the fact that   $\sum_{k \neq l} p_{kl}=1$. Substituting in (\ref{Cgrad}):
$$
 \frac{\partial C}{\partial y_i}= 4\sum_j (p_{ij}- q_{ij})(y_i-y_j)
$$


\section{Proximal gradient descent}
\subsection{A}
$$
\frac{\partial }{\partial z}(\frac{1}{2}||z-x||^2_2+t||z||_1)=z_i-|x|+\mathrm{ sign}(||z||_1)t=0
$$
 Solving for z:
 $$
 z=|x|-\mathrm{ sign}(||z||_1)t
 $$
We have three possible values depending on the value of $z$ being positive, negative or null:
 \begin{eqnarray*}
 z&=&|x| -t \,\,\, \mathrm{for } \,\,x>t\\
 z&=&|x| +t\,\,\, \mathrm{for } \,\,x<-t\\
 z&=&0\,\,\, \mathrm{for } \,\,|x|<t\\
  \end{eqnarray*}
 That is the soft-thresholding function:
 $$
 S_t(x)={(|x|-t)}_+\odot \mathrm{sign}(x)  
 $$
And then for a vector:
 \begin{equation}\label{soft}
 S_t(\mathbf{x})={(|\mathbf{x}|-t)}_+\odot \mathrm{sign}(\mathbf{x})  
 \end{equation}

\subsection{B}
By substituting $\nabla g(x_k) = -X^T(y-Xx_k)$ in:
$$
x_{k+1}=\mathrm{prox}_{h,\alpha_k}(x_k-\alpha_k\nabla g(x_k)) = S_{\alpha_k}(x_k-\alpha_k\nabla g(x_k))
$$
$$
x_{k+1} = S_{\alpha_k}{(x_{k} - \alpha_{k}X^T(Xx_{k}-y))}
$$
This is exactly the update rule used for ISTA, as in~\cite{mairal2014sparse} with a few variable changes.

\subsection{C}
$$
u=\mathrm{prox}_{h,t}\Rightarrow \partial_u[\frac{1}{2}||u-x||^2_2+th(u)] \,\in \,0
$$
$$
u-x+t\partial h(u)\,\in \,0
$$
:
$$
\implies \frac{x-u}{t}\,\in \,\partial h(u)
$$
\subsection{D}
From $G_{\alpha_k}(x_k)$ we know that, 
$$
\mathrm{prox}_{h,t}(x_k-\alpha_k\nabla g(x_k)) =x_k-\alpha_k G_{\alpha_k}(x_k) =x_{k+1}
$$
and 
$$
\mathrm{prox}_{h,t}(x_k-\alpha_k\nabla g(x_k)) =\mathrm{argmin}_{x_{k+1}}(\frac{1}{2}||x_{k+1}-x_k+\alpha_k \nabla g(x_k)||^2_2+th(x_{k+1})) 
$$
$$
\implies \partial(\frac{1}{2}||x_{k+1}-x_k+\alpha_k \nabla g(x_k)||^2_2+th(x_{k+1})) =0
$$
By solving the above equations,
\begin{equation}\label{kplus1}
x_{k+1}=x_k-\alpha_k\nabla g(x_k) -t \partial h(x_{k+1})
\end{equation}
Evaluating
$$
G_{\alpha_k}(x_k)-\nabla g(x_k) = \frac{x_k}{\alpha_k}-\frac{1}{\alpha_k}\mathrm{prox}_{h,t}(x_k-\alpha_k\nabla g(x_k)) -\nabla g(x_k) 
$$
using (\ref{kplus1}):
$$
G_{\alpha_k}(x_k)-\nabla g(x_k)  \in  \frac{x_k}{\alpha_k} -\frac{1}{\alpha_k}(x_k-\alpha_k\nabla g(x_k) -t \partial h(x_{k+1}))
-\nabla g(x_k)
$$
we get:
$$
G_{\alpha_k}(x_k)-\nabla g(x_k)  \in  \partial h(x_{k+1})
$$

\bibliographystyle{plain}
\bibliography{references}

\end{document}
